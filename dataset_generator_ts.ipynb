{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset_generator_ts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMu1rJeg6brrnKnZRjpAycj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezeguins/Bounding-Box-Detecction-VGG16/blob/main/dataset_generator_ts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATE .NPZ DATASET FILE FROM TWO DIFFERENT CVS ANNOTATIONS FILES\n",
        "First dataset genertion and second dataset generation attached to the first one.\n",
        "Record file to current gdrive \n",
        "\n"
      ],
      "metadata": {
        "id": "A3mf1V3HMc7o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-EZ_yWXMYAq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "82v5HELNNBUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/gdrive/MyDrive/VGG16/dataset\"\n",
        "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"images\"])\n",
        "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"etiquetas_export-data-60_solobloques.csv\"])\n",
        "IMAGES_PATH2 = \"/content/gdrive/MyDrive/segundoset/finales\"\n",
        "ANNOTS_PATH2 = \"/content/gdrive/MyDrive/segundoset/etiquetas-segundos-export.csv\"\n",
        "\n",
        "# define the path to the base output directory\n",
        "BASE_OUTPUT = \"/content/gdrive/MyDrive/VGG16/output\"\n",
        "# define the path to the output serialized model, model training plot,\n",
        "# and testing image filenames"
      ],
      "metadata": {
        "id": "Gb7hw7enMxzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the contents of the CSV annotations file\n",
        "print(\"[INFO] loading dataset...\")\n",
        "rows = open(ANNOTS_PATH).read().strip().split(\"\\n\")\n",
        "rows = [w.replace(';', ',') for w in rows]\n",
        "# initialize the list of data (images), our target output predictions (bounding box coordinates), along with the filenames of the\n",
        "# individual images\n",
        "data = []\n",
        "targets = []\n",
        "filenames = []"
      ],
      "metadata": {
        "id": "xV1rEVqlNM1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST DATASET GENERATION\n",
        "\n",
        "# loop over the rows\n",
        "for idx, row in enumerate(rows):\n",
        "\t# break the row into the filename and bounding box coordinates\n",
        "  row = row.split(\",\")\n",
        "  #print(row)\n",
        "  (filename,c,startX,c,  endX,c, startY,c, endY) = row\n",
        "  if startX != \"0\" or startX != \"\" :\n",
        "    print(idx, filename, \"startX\",startX, \"endX\", endX, \"startY\", startY, \"endY\", endY)\n",
        "    # derive the path to the input image, load the image (in OpenCV format), and grab its dimensions\n",
        "    imagePath = os.path.sep.join([IMAGES_PATH, filename])\n",
        "     # image = cv2.imread(imagePath)\n",
        "    #(h, w) = image.shape[:2] - no hace falta porque nuestras anotaciones ya están escaladas\n",
        "    # load the image and preprocess it\n",
        "    image = load_img(imagePath, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "\t  # update our list of data, targets, and filenames\n",
        "    data.append(image)\n",
        "    targets.append((startX, startY, endX, endY))\n",
        "    filenames.append(filename)"
      ],
      "metadata": {
        "id": "k8jjLqPYNQvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INITIALIZE SECOND DATASET:\n",
        "\n",
        "print(\"[INFO] loading dataset...\")\n",
        "rows = open(ANNOTS_PATH2).read().strip().split(\"\\n\")\n",
        "rows = [w.replace(';', ',') for w in rows]\n",
        "\n"
      ],
      "metadata": {
        "id": "odrCrdgwNWTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECOND DATASET GENERATION. APPEND TO FIRST DATASET INCLUDED\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# loop over the rows\n",
        "for idx, row in enumerate(rows):\n",
        "# break the row into the filename and bounding box coordinates\n",
        "  row = row.split(\",\")\n",
        "  #print(row)\n",
        "  (filename,c,startX,c,  endX,c, startY,c, endY) = row\n",
        "  \n",
        "  # derive the path to the input image, load the image (in OpenCV format), and grab its dimensions\n",
        "  imagePath = os.path.sep.join([IMAGES_PATH2, filename])\n",
        "  imagePath = imagePath.replace(\"\\ufeff\",\"\")\n",
        "  my_file = Path(imagePath)\n",
        "  if my_file.is_file():\n",
        "    # image = cv2.imread(imagePath)\n",
        "    #(h, w) = image.shape[:2] - no hace falta porque nuestras anotaciones ya están escaladas\n",
        "    # load the image and preprocess it\n",
        "    if startX != \"0\":\n",
        "      image = load_img(imagePath, target_size=(224, 224))\n",
        "      image = img_to_array(image)\n",
        "      print(idx, filename, \"startX\",startX, \"endX\", endX, \"startY\", startY, \"endY\", endY)\n",
        "\t    # update our list of data, targets, and filenames\n",
        "      data.append(image)\n",
        "      targets.append((startX, startY, endX, endY))\n",
        "      filenames.append(filename)\n",
        "  else:\n",
        "    print(filename, \"no existe\")"
      ],
      "metadata": {
        "id": "BITrLbB-NW_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANNOTATION CONVERSION. FROM STRING TO FLOAT32\n",
        "\n",
        "res = []\n",
        "for tup in targets:\n",
        "  temp = []\n",
        "  for ele in tup:\n",
        "    if ele==\"\":\n",
        "      temp.append(0)\n",
        "    else:\n",
        "      temp.append(float(ele))\n",
        "  res.append((temp[0],temp[1],temp[2],temp[3]))"
      ],
      "metadata": {
        "id": "KIPydhk-NZxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE TO DGRIVE THE DATASET FILE .NPZ\n",
        "\n",
        "inicio = 0\n",
        "fin = len(res)\n",
        "print(fin)\n",
        "# convert the data and targets to NumPy arrays, scaling the input\n",
        "# pixel intensities from the range [0, 255] to [0, 1]\n",
        "data1 = np.array(data[inicio: fin], dtype=\"float32\") / 255.0\n",
        "targets1 = np.array(res[inicio: fin], dtype=\"float32\")\n",
        "filenames1 = filenames[inicio: fin]\n",
        "outfile = \"/content/gdrive/MyDrive/VGG16/dataset/datos.npz\"\n",
        "np.savez(outfile, data1, targets1,filenames1)"
      ],
      "metadata": {
        "id": "kG94_6hGNbpy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}