{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezeguins/Bounding-Box-Detecction-VGG16/blob/main/model_training_ts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngn0q4QoqenY"
      },
      "source": [
        "THE NOTEBOOKS PERFORMS THE FOLLOWING:\n",
        "- Load the dataset generated at \"dataset_generator_ts.ipynb\" file\n",
        "- Initialize model parameters\n",
        "- partition the data into training and testing splits using 90% of the data for training and the remaining 10% for testing\n",
        "- Load the VGG16 network with \"imagenet\" weights, ensuring the head FC layers are left off\n",
        "- Construct the model we will fine-tune for bounding box regression\n",
        "- Initialize and train de model\n",
        "- Plot results and model training history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5w4L3GDpkWs",
        "outputId": "e05d98ea-719a-44e0-abf2-e3b5f1fed328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r74F2yUa4uJP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962870a1-4029-446b-e11b-02a2cc58266d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/VGG16/output/detector20.h5\n"
          ]
        }
      ],
      "source": [
        "version = 20\n",
        "detector = 'detector' + str(version) + '.h5'\n",
        "plot = 'plot' + str(version) + '.png'\n",
        "test_images = 'test_images' + str(version) + '.txt'\n",
        "\n",
        "BASE_PATH = \"/content/gdrive/MyDrive/VGG16/dataset\"\n",
        "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"images\"])\n",
        "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"etiquetas_export-data-60_solobloques.csv\"])\n",
        "\n",
        "# define the path to the base output directory\n",
        "BASE_OUTPUT = \"/content/gdrive/MyDrive/VGG16/output\"\n",
        "# define the path to the output serialized model, model training plot,\n",
        "# and testing image filenames\n",
        "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, detector])\n",
        "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, plot])\n",
        "TEST_FILENAMES = os.path.sep.join([BASE_OUTPUT, test_images])\n",
        "print(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxK9skjc8ZFK"
      },
      "outputs": [],
      "source": [
        "# initialize our initial learning rate, number of epochs to train\n",
        "# for, and the batch size\n",
        "INIT_LR = 1e-4\n",
        "NUM_EPOCHS = 25\n",
        "BATCH_SIZE = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7lltYruqbHa"
      },
      "source": [
        "  TRAIN.PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPpFQ75hpGB6"
      },
      "outputs": [],
      "source": [
        "# import the necessary packages\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outfile = \"/content/gdrive/MyDrive/VGG16/dataset/datos_full.npz\"\n",
        "npzfile = np.load(outfile)\n",
        "data1 = npzfile['arr_0']\n",
        "targets1 = npzfile['arr_1']\n",
        "filenames1 = npzfile['arr_2']"
      ],
      "metadata": {
        "id": "Rrmny8RVPYtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrWs6zaD4hSh",
        "outputId": "94208dc7-8827-4ea3-d1dd-114bfa78d066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] saving testing filenames...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# partition the data into training and testing splits using 90% of\n",
        "# the data for training and the remaining 10% for testing\n",
        "split = train_test_split(data1, targets1, filenames1, test_size=0.10,\trandom_state=42)\n",
        "# unpack the data split\n",
        "(trainImages, testImages) = split[:2]\n",
        "(trainTargets, testTargets) = split[2:4]\n",
        "(trainFilenames, testFilenames) = split[4:]\n",
        "# write the testing filenames to disk so that we can use then\n",
        "# when evaluating/testing our bounding box regressor\n",
        "print(\"[INFO] saving testing filenames...\")\n",
        "f = open(TEST_FILENAMES, \"w\")\n",
        "f.write(\"\\n\".join(testFilenames))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ],
      "metadata": {
        "id": "Y-YBt9yG7aXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83Wl_D1cmFfh"
      },
      "outputs": [],
      "source": [
        "# load the VGG16 network, ensuring the head FC layers are left off\n",
        "vgg = VGG16(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))\n",
        "# freeze all VGG layers so they will *not* be updated during the\n",
        "# training process\n",
        "for layer in vgg.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# flatten the max-pooling output of VGG\n",
        "flatten = vgg.output\n",
        "flatten = Flatten()(flatten)\n",
        "\n",
        "# construct a fully-connected layer header to output the predicted\n",
        "# bounding box coordinates\n",
        "\n",
        "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
        "bboxHead = Dropout(0.2)(bboxHead) # Dropout layer to reduce overfitting\n",
        "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
        "bboxHead = Dense(4, activation=\"sigmoid\")(bboxHead)\n",
        "# construct the model we will fine-tune for bounding box regression\n",
        "model = Model(inputs=vgg.input, outputs=bboxHead)\n",
        "for i, layer in enumerate(model.layers):\n",
        "    print(i, layer.name, layer.trainable)\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=4, verbose=1, mode='max', min_lr=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhgeS44nmXZw"
      },
      "outputs": [],
      "source": [
        "# initialize the optimizer, compile the model, and show the model\n",
        "# summary\n",
        "opt = Adam(lr=INIT_LR)\n",
        "model.compile(loss=\"mse\", optimizer=opt)\n",
        "print(model.summary())\n",
        "# train the network for bounding box regression\n",
        "print(\"[INFO] training bounding box regressor...\")\n",
        "H = model.fit(trainImages, trainTargets,validation_data=(testImages, testTargets),\n",
        "\tbatch_size=BATCH_SIZE,\n",
        "\tepochs=NUM_EPOCHS,\n",
        "\tverbose=1, callbacks=[lr_reduce])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TWg1pBpmuM1"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] saving object detector model...\")\n",
        "model.save(MODEL_PATH, save_format=\"h5\")\n",
        "# plot the model training history\n",
        "N = NUM_EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Bounding Box Regression Loss on Training Set\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(PLOT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZ6L5TEODyeQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "model_training_ts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1v/VvgLSrBFnesJi+Z1vF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}